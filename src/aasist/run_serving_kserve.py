#!/usr/bin/env python3
"""
KServe-based Serving Runner for AASIST Models
Deploy AASIST models using KServe for persistent serving
"""
import os
import sys
import kfp
from pathlib import Path
import argparse

def check_environment():
    """Check if environment is set up correctly"""
    print("üîç Checking environment...")
    
    issues = []
    
    # Check Python version
    if sys.version_info < (3, 8):
        issues.append("Python 3.8+ required")
    
    # Check if KFP is available
    try:
        import kfp
        print(f"  ‚úì Kubeflow Pipelines: {kfp.__version__}")
    except ImportError:
        issues.append("kubeflow-pipelines not installed (pip install kfp)")
    
    if issues:
        print("‚ùå Environment issues found:")
        for issue in issues:
            print(f"  ‚Ä¢ {issue}")
        return False
    else:
        print("‚úÖ Environment looks good!")
        return True

def run_kserve_serving_pipeline(model_path, config_name, service_name, namespace="admin"):
    """Run the KServe-based serving pipeline using KFP client"""
    print("üöÄ Running AASIST KServe Serving Pipeline...")
    
    try:
        from kubeflow_pipeline_serving_fixed import aasist_kserve_serving_pipeline
        
        # Create KFP client
        client = kfp.Client()
        
        # Create or get experiment for multi-user mode
        try:
            experiment_name = "aasist-kserve-experiments"
            experiment = client.create_experiment(
                name=experiment_name,
                description="AASIST KServe serving experiments"
            )
            experiment_id = experiment.id
            print(f"‚úÖ Created new experiment: {experiment_name} (ID: {experiment_id})")
        except Exception as e:
            if "already exists" in str(e).lower():
                experiment = client.get_experiment(experiment_name=experiment_name)
                experiment_id = experiment.id
                print(f"‚úÖ Using existing experiment: {experiment_name} (ID: {experiment_id})")
            else:
                print(f"‚ö†Ô∏è  Could not create/get experiment: {e}")
                print("üîÑ Using default experiment")
                experiment_id = None
        
        # Compile pipeline
        print("üîß Compiling KServe serving pipeline...")
        kfp.compiler.Compiler().compile(
            aasist_kserve_serving_pipeline,
            'aasist_kserve_serving_pipeline.yaml'
        )
        print("‚úÖ KServe serving pipeline compiled successfully!")
        
        # Create run
        print("üöÄ Starting KServe serving pipeline...")
        run_args = {
            'model_path': model_path,
            'config_name': config_name,
            'service_name': service_name,
            'namespace': namespace
        }
        
        run = client.create_run_from_pipeline_func(
            aasist_kserve_serving_pipeline,
            arguments=run_args,
            experiment_id=experiment_id,
            enable_caching=False
        )
        
        print(f"‚úÖ KServe serving pipeline started successfully!")
        print(f"üìä Run ID: {run.run_id}")
        
        if hasattr(run, 'run_url'):
            print(f"üîó View in dashboard: {run.run_url}")
        
        return True
        
    except ImportError as e:
        print(f"‚ùå Pipeline import error: {e}")
        print("üí° Make sure kubeflow_pipeline_serving_fixed.py exists")
        return False
    except Exception as e:
        print(f"‚ùå Pipeline execution error: {e}")
        import traceback
        traceback.print_exc()
        return False

def show_usage_examples():
    """Show usage examples"""
    print("\nüìö Usage Examples:")
    print("\nüîß Example 1: Default (Local model)")
    print("  python run_serving_kserve.py")
    print("  # Uses local trained model in admin namespace")
    
    print("\nüîß Example 2: Custom Model")
    print("  python run_serving_kserve.py --model_path /path/to/model.pth --service_name my-model")
    
    print("\nüîß Example 3: Different Config & Namespace")
    print("  python run_serving_kserve.py --config AASIST-L --namespace kubeflow --service_name large-model")
    
    print("\nüîß Example 4: Custom Service Name")
    print("  python run_serving_kserve.py --service_name aasist-v2")

def show_deployment_info(service_name, namespace):
    """Show deployment information"""
    print(f"\nüéâ KServe deployment process initiated successfully!")
    print(f"‚è±Ô∏è  Monitor progress in the Kubeflow dashboard")
    print(f"\nüìã Service Details:")
    print(f"  ‚Ä¢ Service Name: {service_name}")
    print(f"  ‚Ä¢ Namespace: {namespace}")
    print(f"  ‚Ä¢ Expected URL: http://{service_name}-direct.{namespace}.svc.cluster.local:5000")
    
    print(f"\nüîó Once complete, test with:")
    print(f"  python test.py")
    
    print(f"\nüîß Manual checks:")
    print(f"  kubectl get inferenceservices -n {namespace}")
    print(f"  kubectl get services -n {namespace} | grep {service_name}")
    print(f"  kubectl get pods -n {namespace} | grep {service_name}")

def main():
    """Main entry point"""
    print("ü§ñ AASIST KServe Serving Pipeline Runner")
    print("=" * 45)
    
    # Parse arguments
    parser = argparse.ArgumentParser(description="Deploy AASIST model using KServe")
    parser.add_argument("--model_path", 
                       default="/home/jovyan/mlops/src/aasist/models/weights/AASIST.pth",
                       help="Path to AASIST model file")
    parser.add_argument("--config", dest="config_name", 
                       default="AASIST", choices=["AASIST", "AASIST-L"],
                       help="Model configuration name")
    parser.add_argument("--service_name", 
                       default="aasist-serving",
                       help="Name for the serving service")
    parser.add_argument("--namespace", 
                       default="admin",
                       help="Kubernetes namespace for deployment")
    
    args = parser.parse_args()
    
    # Check environment
    if not check_environment():
        sys.exit(1)
    
    print(f"\nüìã KServe Serving Parameters:")
    print(f"  ‚Ä¢ Model Path: {args.model_path}")
    print(f"  ‚Ä¢ Config: {args.config_name}")
    print(f"  ‚Ä¢ Service Name: {args.service_name}")
    print(f"  ‚Ä¢ Namespace: {args.namespace}")
    
    print("\n" + "=" * 60)
    print("üéØ AASIST KSERVE SERVING PIPELINE")
    print("=" * 60)
    
    print(f"\nüìã What this pipeline does:")
    print(f"  ‚úì Creates persistent KServe InferenceService")
    print(f"  ‚úì Deploys HTTP serving API with proper endpoints")
    print(f"  ‚úì Creates Kubernetes services for access")
    print(f"  ‚úì Tests deployment after creation")
    
    print(f"\n‚ö° Expected runtime: 3-5 minutes")
    print(f"üíæ Resource requirements: 1-2 CPUs, 2-4GB RAM")
    
    print(f"\nüìä Pipeline steps:")
    print(f"  1Ô∏è‚É£  Create KServe InferenceService")
    print(f"  2Ô∏è‚É£  Deploy serving container") 
    print(f"  3Ô∏è‚É£  Create access services")
    print(f"  4Ô∏è‚É£  Test all endpoints")
    
    print(f"\nüîß Configuration options:")
    print(f"  ‚Ä¢ model_path: Path to your trained model")
    print(f"  ‚Ä¢ config_name: 'AASIST' or 'AASIST-L'")
    print(f"  ‚Ä¢ service_name: Name for the serving service")
    print(f"  ‚Ä¢ namespace: Kubernetes namespace")
    
    # Run pipeline
    success = run_kserve_serving_pipeline(
        model_path=args.model_path,
        config_name=args.config_name,
        service_name=args.service_name,
        namespace=args.namespace
    )
    
    if success:
        show_deployment_info(args.service_name, args.namespace)
        show_usage_examples()
        
        print("\nüîÑ For full training workflow, use:")
        print("  ‚Ä¢ python run_demo.py (full demo with dataset)")
        print("  ‚Ä¢ python run_mlflow_serving.py (MLflow integration)")
        
    else:
        print("\n‚ùå KServe serving pipeline execution failed")
        sys.exit(1)

if __name__ == "__main__":
    main() 